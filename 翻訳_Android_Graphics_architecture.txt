Graphics architecture
グラフィック・アーキテクチャ

What every developer should know about Surface, SurfaceHolder, EGLSurface, SurfaceView, GLSurfaceView, SurfaceTexture, TextureView, and SurfaceFlinger
サーフェス、サーフェスホルダー、EGLサーフェス、サーフェスビュー、GLサーフェスビュー、サーフェステスクチャ、テスクチャビュー、サーフェスフリンジャーについてすべての開発者が知っておきべきこと

This document describes the essential elements of Android's "system-level" graphics architecture, and how it is used by the application framework and multimedia system.
このドキュメントはアンドロイドの "システムレベル" グラフィックアーキテクチャの不可欠な要素を記述する。
そして、それが、アプリケーションフレームワークとマルチメディアシステムからどのように使われるかを記述する。

The focus is on how buffers of graphical data move through the system.
フォーカスは、どのようにグラフィックデータのバッファがシステムを通して移動するか。

If you've ever wondered why SurfaceView and TextureView behave the way they do, or how Surface and EGLSurface interact, you've come to the right place.
もし、あなたが、なぜサーフェスビューとテスクチャビューがそのようにふるまうか、もしくはサーフェスとEGLサーフェスがどのように相互作用するかについて疑問を持ったことがあるならば、あなたは正しい場所に来ました。

Some familiarity with Android devices and application development is assumed.
【？】アンドロイドデバイスとアプリケーション開発の親しみやすさが想定される。

You don't need detailed knowledge of the app framework, and very few API calls will be mentioned, but the material herein doesn't overlap much with other public documentation.
アプリフレームワークについての詳細な知識は必要ない。とても少ないいくつかのAPIコールが言及される。しかし、ここの資料はほかの公式ドキュメントとほとんど重複しない。

The goal here is to provide a sense for the significant events involved in rendering a frame for output, so that you can make informed choices when designing an application.
アプリケーションを設計する際に、情報に基づいた選択ができるために、出力のためのフレームの描画に関わる重要なイベントの感覚を提供することを目的とする。

To achieve this, we work from the bottom up, describing how the UI classes work rather than how they can be used.
これを実現するために、ボトムアップでやります。UIクラスが、どのように使われるかではなく、どのように働くかを記述します。

Early sections contain background material used in later sections, so it's a good idea to read straight through rather than skipping to a section that sounds interesting.
最初の方のセクションは、あとのセクションで使われる題材の背景を含むので、最初から(飛ばさずに)読むほうが、興味があるセクションを飛ばし読みするよりも良い考えだ。

We start with an explanation of Android's graphics buffers, describe the composition and display mechanism, and then proceed to the higher-level mechanisms that supply the compositor with data.
アンドロイドのグラフィックバッファの説明、合成と表示のメカニズムの記述から始め、コンポジタとデータを供給するよりハイレベルなメカニズムに進む。

This document is chiefly concerned with the system as it exists in Android 4.4 ("KitKat"). 
このドキュメントは主に Android 4.4 (KitKat)に存在するシステムについて concerned (言及？懸念？) される。

Earlier versions of the system worked differently, and future versions will likely be different as well.
以前のバージョンではシステムの振る舞いは異なり、未来のバージョンでも同様に違うかもしれない。

Version-specific features are called out in a few places.
バージョン依存の機能がいくつかの場所で call out (言及?) される。

At various points I will refer to source code from the AOSP sources or from Grafika.
いくつかの場所で、AOSPソース、もしくはGrafikaからのソースコードを参照する。

Grafika is a Google open source project for testing;
Grafikaはグーグルのテストのためのオープンソースプロジェクト。

it can be found at https://github.com/google/grafika. 
https://github.com/google/grafika にあるよ。
 
It's more "quick hack" than solid example code, but it will suffice.
それはちゃんとしたサンプルコードよりも "quick hack" だが、十分だ。

-----------------------
BufferQueue and gralloc
バッファキューとgralloc
-----------------------

To understand how Android's graphics system works, we have to start behind the scenes.
アンドロイドのグラフィックシステムがどのように動作するか理解するために、シーンの背景から始める必要がある。

At the heart of everything graphical in Android is a class called BufferQueue.
Androidの全てのグラフィックの核心は、バッファキューと呼ばれるクラスだ。

Its role is simple enough:
その役割は実にシンプル。

connect something that generates buffers of graphical data (the "producer") to something that accepts the data for display or further proceグラフィックデータのバッファを生産する何か(生産者)を、そのデータを表示とかほかの処理のために受け付ける何か(消費者)を結びつける。
ssing (the "consumer").

The producer and consumer can live in different processes.
生産者と消費者は異なるプロセスにいてよい。

Nearly everything that moves buffers of graphical data through the system relies on BufferQueue.
システムを通した画像データのバッファの移動のほとんどすべては、バッファキューに依存する。

The basic usage is straightforward.
基本的な使い方は、簡単である。

The producer requests a free buffer (dequeueBuffer()), specifying a set of characteristics including width, height, pixel format, and usage flags.
生産者は、幅、高さ、ピクセルフォーマット、利用法フラグを含む特性のセットを指定して、空バッファを要求する (dequeueBuffer())。

The producer populates the buffer and returns it to the queue (queueBuffer()).
生産者は populates (移入？)して、それをキューに返却する (queueBuffer())。

Some time later, the consumer acquires the buffer (acquireBuffer()) and makes use of the buffer contents.
少しのちに、消費者はバッファを獲得し (acquireBuffer())、そしてバッファのコンテンツを利用する。

When the consumer is done, it returns the buffer to the queue (releaseBuffer()).
消費者が済んだら、バッファをキューに返却する (releaseBuffer())。

Most recent Android devices support the "sync framework".
直近のアンドロイドデバイスは "同期フレームワーク" をサポートする。

This allows the system to do some nifty thing when combined with hardware components that can manipulate graphics data asynchronously.
これはシステムに、複数の画像データを非同期に操作することができるハードウェアコンポーザとの組み合わせで、何か、気の利いたことをするのを、を可能とする。

For example, a producer can submit a series of OpenGL ES drawing commands and then enqueue the output buffer before rendering completes.
たとえば、ある生産者が OpenGL ES 描画コマンドのシリーズをサブミットし、そして、描画が終わる前に、出力バッファに enqueue することができる。

The buffer is accompanied by a fence that signals when the contents are ready.
バッファは内容が準備ができていることを知らせるフェンスを伴っている。

A second fence accompanies the buffer when it is returned to the free list, so that the consumer can release the buffer while the contents are still in use.
次のフェンスは、それがフリーリストに戻されたときに、バッファを伴います。なので、消費者はコンテンツがまだ利用されていても、そのバッファをリリースできる。

This approach improves latency and throughput as the buffers move through the system.
このアプローチは、バッファのシステムを通した移動の、遅延とスループットを改善する。

Some characteristics of the queue, such as the maximum number of buffers it can hold, are determined jointly by the producer and the consumer.
キューのいくつかの特性、バッファを保持できるの最大数など、は生産者と消費者の合意で決定される。

The BufferQueue is responsible for allocating buffers as it needs them.
バッファキューはそれが必要とするバッファのアロケートについて責任を持つ。

Buffers are retained unless the characteristics change;
特性が変化しない限り、バッファが保持される。

for example, if the producer starts requesting buffers with a different size, the old buffers will be freed and new buffers will be allocated on demand.
たとえば、もし生産者が異なるサイズでのバッファの要求を始めたら、古いバッファは破棄され、新しいバッファが必要に応じてアロケートされる。

The data structure is currently always created and "owned" by the consumer.
データ構造は今のところ常に消費者によって作られ、"所持"される。

In Android 4.3 only the producer side was "binderized", i.e. the producer could be in a remote process but the consumer had to live in the process where the queue was created.
Androind 4.3 では生産者側だけが "binderized" である。すなわち、生産者はリモートプロセスになれるが、消費者はキューが生成されたプロセス上で生息しなければならない。

This evolved a bit in 4.4, moving toward a more general implementation.
これは、4.4で、もうすこし一般的な実装に、少し進化した。

Buffer contents are never copied by BufferQueue.
バッファコンテンツはバッファキューによってコピーされることは決してない。

Moving that much data around would be very inefficient.
周りにいる多くのデータを移動することでは効率が悪くなる。

Instead, buffers are always passed by handle.
その代り、バッファは常にハンドルで渡される。

-----------
gralloc HAL
-----------

The actual buffer allocations are performed through a memory allocator called "gralloc", which is implemented through a vendor-specific HAL interface (see hardware/libhardware/include/hardware/gralloc.h). 
実際のバッファアロケーションは "gralloc" と呼ばれるメモリアロケータによって行われる。それは、ベンダー固有のHALインターフェースを通じて実装される。(hardware/libhardware/include/hardware/gralloc.hを見よ)

The alloc() function takes the arguments you'd expect -- width, height, pixel format -- as well as a set of usage flags.
allloc()関数は、利用方法フラグと同様に、あなたが期待する引数、幅、高さ、ピクセルフォーマット、を取る。

Those flags merit closer attention.
【？】これらのフラグは近くで注意深くみるのに値する。

The gralloc allocator is not just another way to allocate memory on the native heap.
【？】grallocアロケータは、ネイティブヒープ上のメモリを割り当てるだけで、別の方法ではない。

In some situations, the allocated memory may not be cache-coherent, or could be totally inaccessible from user space.
いくつかの状況では、割り当てられたメモリはキャッシュコヒーレントではないかもしれない、またはユーザ空間から完全にアクセスできない可能性がある。

The nature of the allocation is determined by the usage flags, which include attributes like:
割り当ての性質は、利用フラグによって決定される。以下のようなアトリビュートを含む：

- how often the memory will be accessed from software (CPU)
そのメモリは、どの程度頻繁にソフトウェア(CPU)からアクセスされるか

- how often the memory will be accessed from hardware (GPU)
そのメモリは、どの程度頻繁にハードウェア(GPU)からアクセスされるか

- whether the memory will be used as an OpenGL ES ("GLES") texture
そのメモリは OpenGLES のテスクチャとして利用されるか

- whether the memory will be used by a video encoder
そのメモリは、ビデオエンコーダで利用されるか

For example, if your format specifies RGBA 8888 pixels, and you indicate the buffer will be accessed from software -- meaning your application will touch pixels directly -- then the allocator needs to create a buffer with 4 bytes per pixel in R-G-B-A order.
たとえば、もし RGBA 8888 フォーマットで、ソフトウェアからアクセスされるバッファの場合 -- つまり アプリがピクセルにダイレクトに触る場合 -- アロケータは R-G-B-Aオーダーの4バイトをピクセルごとに割り当てたバッファをつくる。

If instead you say the buffer will only be accessed from hardware and as a GLES texture, the allocator can do anything the GLES driver wants -- BGRA ordering, non-linear "swizzled" layouts, alternative color formats, etc.
もし、そうでなく、バッファが GLESテスクチャ のようなハードウェアからだけアクセスされるのであれば、アロケータは GLESドライバが要求するいかようにもできる。 --- BGRAオーダー、非線形な swizzled なレイアウト、代替カラーフォーマット、などなど。

Allowing the hardware to use its preferred format can improve performance.
ハードウェアに、パフォーマンスを向上できる好みのフォーマット、を使うことを許可できる。
 
Some values cannot be combined on certain platforms.
一定のプラットフォームと組み合わせることができない値もある。

For example, the "video encoder" flag may require YUV pixels, so adding "software access" and specifying RGBA 8888 would fail.
たとえば、"ビデオエンコーダ"フラッグは YUV ピクセルを要求するかもしれないので、そうだとしたら、"ソフトウェアアクセス"と RGBA 8888 の指定は失敗するだろう。

The handle returned by the gralloc allocator can be passed between processes through Binder.
grallocアロケータから渡されたハンドルは、Binderを使ってプロセッサ間で受け渡しできる。

====================================
SurfaceFlinger and Hardware Composer
サーフェスフリンジャーとハードウェアコンポーザ
====================================

Having buffers of graphical data is wonderful, but life is even better when you get to see them on your device's screen. 
グラフィックデータのバッファを持つことは素晴らしい、しかし、それらをあなたのデバイスの画面で見られれば、もし人生はもっと良くなる。

That's where SurfaceFlinger and the Hardware Composer HAL come in.
それが、サーフェスフリンジャーとハードウェアコンポーザが出てくる where (場所？)。

SurfaceFlinger's role is to accept buffers of data from multiple sources, composite them, and send them to the display.
サーフェスフリンジャーの役割は複数のソースからのデータのバッファを受け取り、それらを合成し、それらを画面に送ること。

Once upon a time this was done with software blitting to a hardware framebuffer (e.g. /dev/graphics/fb0), but those days are long gone.
かつて、これは、ハードウェアのフレームバッファに対してソフトウェアがブリットすることで行われていた (/dev/graphics/fb0)が、それは昔のこと。

When an app comes to the foreground, the WindowManager service asks SurfaceFlinger for a drawing surface.
アプリがフォアグラウンドに来たとき、ウィンドウマネージャサービスはサーフェスフリンジャーにサーフェス描画をお願いする。

SurfaceFlinger creates a "layer" - the primary component of which is a BufferQueue - for which SurfaceFlinger acts as the consumer.
【？】サーフェスフリンジャーは "レイヤ" - それの主要なコンポーネントはバッファキュー - を作成する。サーフェスフリンジャーはコンシューマとしてふるまう。

A Binder object for the producer side is passed through the WindowManager to the app, which can then start sending frames directly to SurfaceFlinger.
生産者側用のバインダーオブジェクトはウィンドウマネージャを通して、アプリに渡される。そして、アプリはサーフェスフリンジャーにダイレクトにフレームを送り始めることができる。

-----
Note: The WindowManager uses the term "window" instead of "layer" for this and uses "layer" to mean something else.
注意：ウインドウマネージャはレイヤの代わりにウィンドウという言葉を使う。レイヤーは何かほかのものを意味するために使う。

We're going to use the SurfaceFlinger terminology.
我々はサーフェスフリンジャーの用語を使うつもり。

It can be argued that SurfaceFlinger should really be called LayerFlinger.
サーフェスフリンジャーは本当はレイヤーフリンジャーと呼ばれるべきと主張できる。
-----

For most apps, there will be three layers on screen at any time: the "status bar" at the top of the screen, the "navigation bar" at the bottom or side, and the application's UI.
たいていのアプリは、つねに3つのレイヤを画面上に持つ："ステータスバー"を画面の上に、"ナビゲーションバー"を下か横に、そして、アプリのUI。

Some apps will have more or less, e.g. the default home app has a separate layer for the wallpaper, while a full-screen game might hide the status bar.
もっと多い、少ない、レイヤを持つアプリもある。たとえば、デフォルトのホームアプリは壁紙用の分離(独立)レイヤを持つし、フルスクリーンでのゲーム中はステータスバーは隠れるだろう。

Each layer can be updated independently.
それぞれのレイヤは独立に更新される。

The status and navigation bars are rendered by a system process, while the app layers are rendered by the app, with no coordination between the two.
ステータスバーとナビゲーションバーはシステムプロセスにより描画され、同時にアプリレイヤはアプリによって描画される。それらの間に連絡はない。

Device displays refresh at a certain rate, typically 60 frames per second on phones and tablets.
デバイスの画面は一定のレートでリフレッシュされる。スマホやタブレットだと、たいてい60フレーム/秒。

If the display contents are updated mid-refresh, "tearing" will be visible; so it's important to update the contents only between cycles.
もし表示コンテンツが、半分だけアップデートされていると、"腹切り"が見える：なので、サイクル間(の時間)だけでコンテンツをアップデートするのが重要となる。

The system receives a signal from the display when it's safe to update the contents.
システムは、安全にコンテンツのアップデートができる時に、ディスプレイからシグナルを受け取る。

For historical reasons we'll call this the VSYNC signal.
歴史上の理由から、我々はこれを VSYNC シグナル、と呼んでいる。
 
The refresh rate may vary over time, e.g. some mobile devices will range from 58 to 62fps depending on current conditions.
リフレッシュレートは経時的に変化するかもしれない。たとえば、状況に依存して、58から62fpsの幅で変化するデバイスもある。

For an HDMI-attached television, this could theoretically dip to 24 or 48Hz to match a video.
HDMI付きのテレビでは、理論的には、ビデオにマッチさせるために、24Hz か 48Hz。

Because we can update the screen only once per refresh cycle, submitting buffers for display at 200fps would be a waste of effort as most of the frames would never be seen.
リフレッシュサイクル中に一回しか画面を更新できないので、200fpsディスプレイ用のバッファを用意するのは無駄な努力だ、多くのフレームは見られることはないだろうから。

Instead of taking action whenever an app submits a buffer, SurfaceFlinger wakes up when the display is ready for something new.
アクションを取る代わりに、アプリはいつでのバッファをサブミットできる、サーフェスフリンジャーはディスプレイが何か新しいもののために準備ができた時に起きる。

When the VSYNC signal arrives, SurfaceFlinger walks through its list of layers looking for new buffers.
VSYNCシグナルが来たときに、サーフェスフリンジャーはレイヤのリストを歩き回り、新しいバッファを探す。

If it finds a new one, it acquires it; if not, it continues to use the previously-acquired buffer.
もし新しいのが見つかれば、それを取得する。もし見つからなければ、以前取得したバッファを使い続ける。

SurfaceFlinger always wants to have something to display, so it will hang on to one buffer.
サーフェスフリンジャーはいつでも何か表示するものを持っていたい。なので、ひとつのバッファの上にまたがっている。

If no buffers have ever been submitted on a layer, the layer is ignored.
一つもバッファがサブミットされていないレイヤがあったら、それは無視される。

Once SurfaceFlinger has collected all of the buffers for visible layers, it asks the Hardware Composer how composition should be performed.
一度サーフェスフリンジャーがすべての可視レイヤのバッファを収集したならば、ハードウェアコンポーザに合成はどう実行されるべきか尋ねる。

-----------------
Hardware Composer
ハードウェアコンポーザ
-----------------

The Hardware Composer HAL ("HWC") was first introduced in Android 3.0 ("Honeycomb") and has evolved steadily over the years.
ハードウェアコンポーザ(HWC)は Android 3.0 (Honeycomb)で最初に導入され、年を重ね、確実に進化してきた。

Its primary purpose is to determine the most efficient way to composite buffers with the available hardware. 
その第一の目的は、利用できるハードウェアに対し、バッファを合成する最も効果的な方法を、決定することである。

As a HAL, its implementation is device-specific and usually implemented by the display hardware OEM.
HALについては、実装はデバイス固有で、たいていの場合、ディスプレイハードウェアOEMにより実装される。

The value of this approach is easy to recognize when you consider "overlay planes." 
"重複プレーン"を考えた時に、このアプローチの価値は簡単にわかる。

The purpose of overlay planes is to composite multiple buffers together, but in the display hardware rather than the GPU.
重複プレーンの目的は、複数のバッファを互いに合成すること。しかし、ディスプレイハードウェアではなく、むしろGPUを使って。

For example, suppose you have a typical Android phone in portrait orientation, with the status bar on top and navigation bar at the bottom, and app content everywhere else.
たとえば、あなたは典型的な縦型画面のアンドロイドフォンを持っているだろう、ステータスバーが上にあり、ナビゲーションバーが下にあり、他のところにはアプリのコンテンツがある。

The contents for each layer are in separate buffers.
それぞれのレイヤのコンテンツは分離したバッファの中にある。

You could handle composition by rendering the app content into a scratch buffer, then rendering the status bar over it, then rendering the navigation bar on top of that, and finally passing the scratch buffer to the display hardware.
あなたが合成を取り扱うこともできる、アプリのコンテンツをスクラッチのバッファに描画し、それからステータスバーをその上に描画し、それからナビゲーションバーをその上に描画し、最後にその引き延ばされたバッファを描画ハードウェアに渡す。

Or, you could pass all three buffers to the display hardware, and tell it to read data from different buffers for different parts of the screen.
もしくは、あなたはこれらの3つのバッファを描画ハードウェアにわたし、それぞれのバッファからデータを読んで、画面上の別の場所に書いて、とお願いすることもできる。

The latter approach can be significantly more efficient.
後者のアプローチはかなりより効率的にすることができる。

As you might expect, the capabilities of different display processors vary significantly.
予測されるように、描画プロセッサが異なれば、その描画能力はかなり違う。

The number of overlays, whether layers can be rotated or blended, and restrictions on positioning and overlap can be difficult to express through an API.
オーバーレイの数、レイヤーに対して回転やブレンドができるかどうか、場所と重なりの制約はAPIを通して表現されづらい。

So, the HWC works like this:
なので、HWCはこのように働く：

1. SurfaceFlinger provides the HWC with a full list of layers, and asks, "how do you want to handle this?"
サーフェスフリンジャーはHECにすべてのレイヤのリストを渡し、「どういう風に扱いたい？」と聞く

2. The HWC responds by marking each layer as "overlay" or "GLES composition."
HWCは各レイヤに "オーバーレイ" もしくは "GLES composition"というマークをつけて答える

3. SurfaceFlinger takes care of any GLES composition, passing the output buffer to HWC, and lets HWC handle the rest.
サーフェスフリンジャーは GLES composition の面倒を見る、出力バッファをHWCにわたし、残りをHWCに処理させる。

Since the decision-making code can be custom tailored by the hardware vendor, it's possible to get the best performance out of every device.
なので、判断決定コードはハードウェアベンダーによるカスタマイズされたオーダーメードにでき、それぞれのデバイスについて最適なパフォーマンスを得ることを可能としている。

Overlay planes may be less efficient than GL composition when nothing on the screen is changing.
なにも画面上の変化がないとき、重畳プレーンは GL composition　にたいして効率が劣るかもしれない。

This is particularly true when the overlay contents have transparent pixels, and overlapping layers are being blended together.
これは、重畳コンテンツが透過ピクセルをもち、オーバーラップされたレイヤが互いにブレンドされているときに、特に真実(顕著)

In such cases, the HWC can choose to request GLES composition for some or all layers and retain the composited buffer.
そのようなケースで、HWCは、いくつか、もしくはすべてのレイヤーについて GLES composition を要求することを選択でき、合成済みのバッファを保持できる。

If SurfaceFlinger comes back again asking to composite the same set of buffers, the HWC can just continue to show the previously-composited scratch buffer.
もしサーフェスフリンジャーがふたたび同じバッファのセットの合成をお願いする場合には、HWCは以前合成されたスクラッチバッファを表示し続けるだけでよい。

This can improve the battery life of an idle device.
これはアイドリング状態のデバイスのバッテリ寿命を改善する。
 
Devices shipping with Android 4.4 ("KitKat") typically support four overlay planes. 
Android 4.4 (KitKat)とともに販売されるデバイスは一般的に４つの重畳プレーンをサポートする。

Attempting to composite more layers than there are overlays will cause the system to use GLES composition for some of them; so the number of layers used by an application can have a measurable impact on power consumption and performance.
存在するよりも多くのレイヤを合成する試みは、それらのいくつかに対してシステムが GLES合成 を利用することの要因となる。なので、アプリケーションから使われるレイヤの数は消費電力とパフォーマンスについて主要なインパクトを持つ。

You can see exactly what SurfaceFlinger is up to with the command adb shell dumpsys SurfaceFlinger.
adb shell dumpsys SurfaceFlinger というコマンドを使って、サーフェスフリンジャーが何をやっているか正確に知ることができる。

The output is verbose.
出力は冗長。

The part most relevant to our current discussion is the HWC summary that appears near the bottom of the output:
本議論に最も関係する部分は、出力の最後の HWC summary の部分。

(adb shell dumpsys SufraceFlingerの出力画面)

This tells you what layers are on screen, whether they're being handled with overlays ("HWC") or OpenGL ES composition ("GLES"), and gives you a bunch of other facts you probably won't care about ("handle" and "hints" and "flags" and other stuff that we've trimmed out of the snippet above). 
これはあなたに、どんなレイヤが画面上にあるか、それらがオーバーレイ(HWC)とOpenGL ES合成(GLES)のどちらで扱われているか、さらに、あなたが多分あまり気にしない事実の束(我々が上記の引用から削った "handle"、"hints"、"flags" やほかの項目)、を知らせる。

The "source crop" and "frame" values will be examined more closely later on.
"source crop"と"frame"の値については、後程、詳細に検証する。

The FB_TARGET layer is where GLES composition output goes.
FB_TARGETレイヤは、GLES合成の出力が出ていくところ。

Since all layers shown above are using overlays, FB_TARGET isn’t being used for this frame.
上記の例ではすべてのレイヤがオーバーレイを利用しているので、FB_TARGETは利用されていない。

The layer's name is indicative of its original role: On a device with /dev/graphics/fb0 and no overlays, all composition would be done with GLES, and the output would be written to the framebuffer.
そのレイヤ名はそれのもともとの役割を示している。デバイス上の /dev/graphics/fb0 と 非オーバーレイ、すべての合成はGLESによってなされる。出力はフレームバッファに書かれる。

On recent devices there generally is no simple framebuffer, so the FB_TARGET layer is a scratch buffer.
最近のデバイスはシンプルなフレームバッファではないため、FB_TARGETレイヤはスクラッチバッファである。

-----
Note: This is why screen grabbers written for old versions of Android no longer work: 
これが古くバージョンのアンドロイド用の画面キャプチャが動作しない理由。

They're trying to read from the Framebuffer, but there is no such thing.
それらはフレームバッファから読み込もうとするが、そもそもそんなものはない。
-----

The overlay planes have another important role: 
重畳プレーンはほかの重要な役割を持っている。

they're the only way to display DRM content.
DRMコンテンツを表示する唯一の方法。

DRM-protected buffers cannot be accessed by SurfaceFlinger or the GLES driver, which means that your video will disappear if HWC switches to GLES composition.
サーフェスフリンジャーやGLESドライバは、DRM保護がされたバッファにアクセスできない。それはもしHWCがGLES合成に切り替わったら画が消えることを意味する。

-----------------------------
The Need for Triple-Buffering
３重バッファの必要性
-----------------------------

To avoid tearing on the display, the system needs to be double-buffered: the front buffer is displayed while the back buffer is being prepared.
画面上の腹切りを避けるために、システムは２重バッファを必要とした：フロントバッファが表示されている間に、バックバッファが用意される。

At VSYNC, if the back buffer is ready, you quickly switch them.
VSYNCの時に、もしバックバッファの準備ができていれば、迅速にそれにスイッチする。

This works reasonably well in a system where you're drawing directly into the framebuffer, but there's a hitch in the flow when a composition step is added.
この仕組みは、フレームバッファに直接書き込むシステムでは、合理的に良く動作していた。しかし
Because of the way SurfaceFlinger is triggered, our double-buffered pipeline will have a bubble. しかし合成のステップが追加されると、フローに邪魔が入る。

Suppose frame N is being displayed, and frame N+1 has been acquired by SurfaceFlinger for display on the next VSYNC.
フレームNが描画されていると仮定すると、N+1フレームは次のVSYNCで表示されるために、サーフェスフリンジャーに獲得されている。

(Assume frame N is composited with an overlay, so we can't alter the buffer contents until the display is done with it.)
 Nがオーバーレイで合成されていると仮定すると、それによって描画が完了するまで、コンテンツ・バッファをを変更できない。

When VSYNC arrives, HWC flips the buffers.
VSYNCが来ると、HWCはバッファをフリップする。

While the app is starting to render frame N+2 into the buffer that used to hold frame N, SurfaceFlinger is scanning the layer list, looking for updates.
アプリケーションが、かつてフレームNに保持されていた、N+2フレームのバッファに描画をはじめたとき、サーフェスフリンジャーが更新確認のためにレイヤのリストをスキャンする。

SurfaceFlinger won't find any new buffers, so it prepares to show frame N+1 again after the next VSYNC.
サーフェスフリンジャーはいかなる新バッファも発見せず、よって、次のVSYNCでN+1フレームを再度描画する準備をする。

A little while later, the app finishes rendering frame N+2 and queues it for SurfaceFlinger, but it's too late.
その少し後に、アプリはN+2フレームへの描画を終え、それをサーフェスフリンジャーにキューする。が、遅すぎた。

This has effectively cut our maximum frame rate in half.
これは、効果的に最大フレームレートを半分にカットする。

We can fix this with triple-buffering.
我々は３重バッファでこれを修正できる。

Just before VSYNC, frame N is being displayed, frame N+1 has been composited (or scheduled for an overlay) and is ready to be displayed, and frame N+2 is queued up and ready to be acquired by SurfaceFlinger.
VSYNCのちょうど前、フレームNは表示されていて、フレームN+1は合成(もしくはオーバーレイがスケジュール)され、表示できる状態にあり、フレームN+2はキューに積まれサーフェスフリンジャーが取得できるようになっている。

When the screen flips, the buffers rotate through the stages with no bubble.
画面がフリップすると、バッファはステージを bubble なく、ローテ―トする。

The app has just less than a full VSYNC period (16.7ms at 60fps) to do its rendering and queue the buffer.
アプリは 描画とバッファのキューイン具について、VSYNCの間隔(60fpsの場合16.7ミリ秒)未満の時間を持つ。

And SurfaceFlinger / HWC has a full VSYNC period to figure out the composition before the next flip.
サーフェスフリンジャーとHWCは、次のフリップ前の合成に、VSYNC間隔と同じ時間を持つ。

The downside is that it takes at least two VSYNC periods for anything that the app does to appear on the screen.
マイナス面として、アプリが何を行っても、それが画面に出るのに、少なくとも 2 VSYNC の期間がかかる。

As the latency increases, the device feels less responsive to touch input.
この遅延の増加で、デバイスは、タッチイベントに対するレスポンスが落ちたと感じる。

Figure 1. SurfaceFlinger + BufferQueue
図１、サーフェスフリンジャーとバッファキュー

The diagram above depicts the flow of SurfaceFlinger and BufferQueue. 
上の図はサーフェスフリンジャーとバッファキューのフローを書いている。

During frame:
(いち)フレームの間に：

1. red buffer fills up, then slides into BufferQueue
赤いバッファが埋められて、バッファキューに渡される。

2. after red buffer leaves app, blue buffer slides in, replacing it
赤いバッファがアプリを去ると、青いバッファが代わりに入ってくる。

3. green buffer and systemUI* shadow-slide into HWC (showing that SurfaceFlinger still has the buffers, but now HWC has prepared them for display via overlay on the next VSYNC).
緑のバッファとシステムUI(*)のシャドースライドがHWCに入る (サーフェスフリンジャーがバッファをまだ持っているが、HWCは次のVSYNCで重畳表示するためにそれらを準備する)

The blue buffer is referenced by both the display and the BufferQueue. 
青いバッファはディスプレイとバッファキューの両方から参照されている。

The app is not allowed to render to it until the associated sync fence signals.
アプリケーションは関連付けられた同期フェンス信号まで描画ができない。

On VSYNC, all of these happen at once:
VSYNCで、これらすべてのことが一度に起こる：

- red buffer leaps into SurfaceFlinger, replacing green buffer
赤いバッファがサーフェスフリンジャーに飛んでいき、緑のバッファを置き換える。

- green buffer leaps into Display, replacing blue buffer, and a dotted-line green twin appears in the BufferQueue
緑のバッファはディスプレイに飛んでいき、青いバッファを置き換える。ドット線で結ばれた緑の双子がバッファキューに現れる。

- the blue buffer’s fence is signaled, and the blue buffer in App empties**
青いバッファのフェンスはシグナルされ、アプリ中の青バッファは空になる(**)

- display rect changes from <blue + SystemUI> to <green + SystemUI>
画面の矩形波 青＋システムUI から 緑＋システムUI に変わる

* - The System UI process is providing the status and nav bars, which for our purposes here aren’t changing,
* - システムUIプロセスはステータスバーとナビバーを提供している。我々の目的のために、ここでは変更がない。

so SurfaceFlinger keeps using the previously-acquired buffer.
サーフェスフリンジャーは、以前獲得したバッファの利用を続ける。

In practice there would be two separate buffers, one for the status bar at the top, one for the navigation bar at the bottom, and they would be sized to fit their contents.
実際には、二つのバッファがある。一つは画面上のステータスバー用。もうひとつは画面下のナビバー用。そしてそれらはコンテンツにフィットするサイズになっている。

Each would arrive on its own BufferQueue.
それぞれが、それぞれが持つバッファキューに現れる。

** - The buffer doesn’t actually “empty”; if you submit it without drawing on it you’ll get that same blue again. 
** - バッファは実際には「空」ではない。もし何も描画しないでサブミットしたら、同じ青の絵を再び得ることになる。

The emptying is the result of clearing the buffer contents, which the app should do before it starts drawing.
「空になる」のは、コンテンツ・バッファのクリアの結果であり、アプリは描画の前にこれをすべきである。

We can reduce the latency by noting layer composition should not require a full VSYNC period.
我々は、全VSYNC時間を必要としない、レイヤの合成に着目して、遅延を削減できる。

If composition is performed by overlays, it takes essentially zero CPU and GPU time.
もし合成がオーバーレイによって行われれば、基本的にCPUもGPUも消費時間ゼロである。

But we can't count on that, so we need to allow a little time.
しかし、我々はそれに頼ることはできない。なので、少しの時間は許可する必要がある。

If the app starts rendering halfway between VSYNC signals, and SurfaceFlinger defers the HWC setup until a few milliseconds before the signal is due to arrive, we can cut the latency from 2 frames to perhaps 1.5.
もし、アプリがVSYNCの途中で描画を開始した場合、サーフェスフリンジャーはHWCのセットアップを、シグナルの到着予測より数ミリ秒前まで遅延する。われわれは遅延を2からたぶん1.5フレームに削減できる。

In theory you could render and composite in a single period, allowing a return to double-buffering; but getting it down that far is difficult on current devices.
理論的には、描画と合成が一回の間隔で収まれば、ダブルバッファの世界に戻れる。しかし、いまのデバイスでは大変難しいので、それは取り下げた。

Minor fluctuations in rendering and composition time, and switching from overlays to GLES composition, can cause us to miss a swap deadline and repeat the previous frame.
描画と合成の微妙な変動、そしてオーバーレイからGLESへのスイッチが、スワップのデッドラインに間に合わずに、前のフレームを繰り返す原因の可能性として存在する。

SurfaceFlinger's buffer handling demonstrates the fence-based buffer management mentioned earlier.
サーフェスフリンジャーのバッファ管理は、前述したフェンスベースの管理を言及している。

If we're animating at full speed, we need to have an acquired buffer for the display ("front") and an acquired buffer for the next flip ("back").
もしフルスピードでアニメーションすると、描画用のバッファ(フロント)の取得と、次のフリップ用のバッファ(バック)の取得が必要となる。

If we're showing the buffer on an overlay, the contents are being accessed directly by the display and must not be touched.
バッファをオーバーレイで表示しているとき、コンテンツはディスプレイから直接アクセスされるので、触ってはならない。

But if you look at an active layer's BufferQueue state in the dumpsys SurfaceFlinger output, you'll see one acquired buffer, one queued buffer, and one free buffer.
しかしもし、dumpsys SurfaceFlingerの出力でアクティブレイヤのバッファキューのステートを見るならば、ひとつの取得されたバッファと、ひとつのキューされたバッファと、ひとつの解放されたバッファを見るだろう。

That's because, when SurfaceFlinger acquires the new "back" buffer, it releases the current "front" buffer to the queue.
それは、サーフェスフリンジャーが新しいバックバッファを取得した時、それが現在のフロントバッファをキューにリリースしているからである。

The "front" buffer is still in use by the display, so anything that dequeues it must wait for the fence to signal before drawing on it.
フロントバッファはまだディスプレイによって使われているので、それをデキューするものは何であれ、フェンスがシグナルされるのを、それに描画する前に待たなければならない。

So long as everybody follows the fencing rules, all of the queue-management IPC requests can happen in parallel with the display.
全員がフェンスのルールに従う限り、キュー管理のすべては、IPC要求が表示と並行して発生することがある。

----------------
Virtual Displays
仮想ディスプレイ
----------------

SurfaceFlinger supports a "primary" display, i.e. what's built into your phone or tablet, and an "external" display, such as a television connected through HDMI.
サーフェスフリンジャーはスマホやタブレットに組み込まれた"第一の"ディスプレイと、HDMIで接続されたテレビ等の"拡張"ディスプレイをサポートしている。

It also supports a number of "virtual" displays, which make composited output available within the system.
また、システム内で利用可能な合成されたアウトプットを作り出す、"仮想"ディスプレイもサポートしている。

Virtual displays can be used to record the screen or send it over a network.
仮想ディスプレイは画面を録画したり、ネットワーク越しに送ったりするのに利用できる。

Virtual displays may share the same set of layers as the main display (the "layer stack") or have its own set.
仮想ディスプレイはメインディスプレイと同じレイヤーのセット(レイヤスタック)を共有したり、独自のセットを持ったりする。

There is no VSYNC for a virtual display, so the VSYNC for the primary display is used to trigger composition for all displays.
仮想ディスプレイにVSYNCはない。なので、主画面用のVSYNCをすべてのディスプレイの合成のトリガとする。

In the past, virtual displays were always composited with GLES.
過去には、仮想ディスプレイは常に GLES で合成されていた。

The Hardware Composer managed composition for only the primary display.
ハードウェアコンポーザは主ディスプレイの合成のみを管理していた。

In Android 4.4, the Hardware Composer gained the ability to participate in virtual display composition.
Android 4.4で、ハードウェアコンポーザは仮想ディスプレイの合成に参加する能力を得た。

As you might expect, the frames generated for a virtual display are written to a BufferQueue.
あなたが期待しているように、仮想ディスプレイ用に生成されたフレームはバッファキューに書かれる。

------------------------
Case study: screenrecord
ケーススタディ：screenrecord
------------------------

Now that we've established some background on BufferQueue and SurfaceFlinger, it's useful to examine a practical use case.
さて我々はバッファキューとサーフェスフリンジャーについての後ろ盾を確立した。それは実用的なユースケースの調査の役に立つ。

The screenrecord command, introduced in Android 4.4, allows you to record everything that appears on the screen as an .mp4 file on disk.
Android4.4で導入された、screenrecordコマンドは、あたたが画面に表示されているすべてのものを、ディスク上の .mp4 ファイルに記録することを可能とする。

To implement this, we have to receive composited frames from SurfaceFlinger, write them to the video encoder, and then write the encoded video data to a file.
これを実現するために、サーフェスフリンジャーから合成されたフレームを受け取り、それをビデオエンコーダに書き込み、エンコードされたビデオをファイルに書き込む必要がある。

The video codecs are managed by a separate process - called "mediaserver" - so we have to move large graphics buffers around the system.
ビデオコーデックは mediaserver という別プロセスで管理されている、よって大きなグラフィックバッファをシステムの周りで動かさなければならない。

To make it more challenging, we're trying to record 60fps video at full resolution.
もっとチャレンジングにしているのは、我々はフル解像度の60fpsのビデオの記録に挑戦している。

The key to making this work efficiently is BufferQueue.
この仕事を効果的にする鍵はバッファキューです。

The MediaCodec class allows an app to provide data as raw bytes in buffers, or through a Surface.
MediaCodecクラスはアプリにバッファ中の生バイト列、もしくはサーフェスを通してデータを供給させる。

We'll discuss Surface in more detail later, but for now just think of it as a wrapper around the producer end of a BufferQueue.
サーフェスについての詳細な議論は後にして、今はそれをバッファキューの生産者側のラッパだと考えよう。

When screenrecord requests access to a video encoder, mediaserver creates a BufferQueue and connects itself to the consumer side, and then passes the producer side back to screenrecord as a Surface.
screenrecordがビデオエンコーダにアクセスを要求すると、mediaserverはバッファキューを生成し、その消費者側に自分自身を接続し、生産者側をscreenrecordにサーフェスとして渡す。

The screenrecord command then asks SurfaceFlinger to create a virtual display that mirrors the main display (i.e. it has all of the same layers), and directs it to send output to the Surface that came from mediaserver.
つぎに、screenrecordコマンドは、サーフェスフリンジャーにメインディスプレイのミラーとなる仮想ディスプレイの生成を要求する(それはすべての同じレイヤを持つ)。そして、メディアサーバから来たサーフェスにダイレクトに送出するようにする。

Note that, in this case, SurfaceFlinger is the producer of buffers rather than the consumer.
なので、このケースでは、サーフェスフリンジャーは消費者というよりは、バッファの生産者である。

Once the configuration is complete, screenrecord can just sit and wait for encoded data to appear.
いったんコンフィグレーションが終われば、screenrecordはエンコードされたデータが現れるのをただ座って待つだけである。

As apps draw, their buffers travel to SurfaceFlinger, which composites them into a single buffer that gets sent directly to the video encoder in mediaserver.
アプリが描くように、それらのバッファが、 MediaServerの中にビデオエンコーダに直接送信される単一のバッファにそれらを合成する、 SurfaceFlingerへ移動する。

The full frames are never even seen by the screenrecord process. 
全フレームはscreenrecordプロセスから見えることはない。

Internally, mediaserver has its own way of moving buffers around that also passes data by handle, minimizing overhead.
内部的に、mediaserverは、バッファを動かす独自の方法を持っている。それはオーバーヘッドを最小化するためにハンドルでデータを渡す。

---------------------------------------
Case study: Simulate Secondary Displays
---------------------------------------

The WindowManager can ask SurfaceFlinger to create a visible layer for which SurfaceFlinger will act as the BufferQueue consumer. It's also possible to ask SurfaceFlinger to create a virtual display, for which SurfaceFlinger will act as the BufferQueue producer. What happens if you connect them, configuring a virtual display that renders to a visible layer?

You create a closed loop, where the composited screen appears in a window. Of course, that window is now part of the composited output, so on the next refresh the composited image inside the window will show the window contents as well. It's turtles all the way down. You can see this in action by enabling "Developer options" in settings, selecting "Simulate secondary displays", and enabling a window. For bonus points, use screenrecord to capture the act of enabling the display, then play it back frame-by-frame.
